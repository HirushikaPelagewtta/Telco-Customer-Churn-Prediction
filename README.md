
---

# ğŸ“Š Telco Customer Churn Prediction

This project focuses on predicting **customer churn** in the telecommunications sector using **data preprocessing, feature engineering, and machine learning models**. The repository includes exploratory data analysis (EDA), model training and evaluation, and full pipeline scripts for deployment and inference.

---

## ğŸš€ Features

* **EDA**: Handling missing values, outliers, feature binning, encoding, and scaling.
* **Model Implementation & Evaluation**:

  * Base model training
  * K-Fold cross-validation
  * Multi-model training and comparison
  * Hyperparameter tuning
  * Threshold optimization
  * Best model selection and usage
* **Pipelines**:

  * Data pipeline
  * Training pipeline
  * Streaming inference pipeline
* **Reusable Modules**: Modularized preprocessing, feature engineering, model building, training, evaluation, and inference.

---

## ğŸ“‚ Repository Structure

```
TELCO-CUSTOMER-CHURN-PREDICTION
â”‚
â”œâ”€â”€ EDA/                           # Jupyter notebooks for data preprocessing & EDA
â”‚   â”œâ”€â”€ 0_Handling_Missing_Values.ipynb
â”‚   â”œâ”€â”€ 1_Handling_Outliers.ipynb
â”‚   â”œâ”€â”€ 2_Feature_Binning.ipynb
â”‚   â”œâ”€â”€ 3_Encoding_And_Scaling.ipynb
â”‚   â””â”€â”€ requirments.txt
â”‚
â”œâ”€â”€ Model_Implementation_andEvaluation/   # Model training & evaluation
â”‚   â”œâ”€â”€ 1_Base_Model_Training.ipynb
â”‚   â”œâ”€â”€ 2_kfold_validation.ipynb
â”‚   â”œâ”€â”€ 3_Multi_model_training.ipynb
â”‚   â”œâ”€â”€ 4_Hyperparam_tuning.ipynb
â”‚   â”œâ”€â”€ 5_Threshold_optimization.ipynb
â”‚   â””â”€â”€ 6_Using_the_Best_Model.ipynb
â”‚
â”œâ”€â”€ Python_Script/
â”‚   â”œâ”€â”€ pipelines/                 # ML pipelines
â”‚   â”‚   â”œâ”€â”€ data_pipeline.py
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â”‚   â””â”€â”€ streaming_inference_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ src/                       # Core reusable modules
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_splitter.py
â”‚   â”‚   â”œâ”€â”€ feature_binning.py
â”‚   â”‚   â”œâ”€â”€ feature_encoding.py
â”‚   â”‚   â”œâ”€â”€ feature_scaling.py
â”‚   â”‚   â”œâ”€â”€ handle_missing_values.py
â”‚   â”‚   â”œâ”€â”€ outlier_detection.py
â”‚   â”‚   â”œâ”€â”€ model_building.py
â”‚   â”‚   â”œâ”€â”€ model_training.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â””â”€â”€ model_inference.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # Configurations & helper functions
â”‚       â”œâ”€â”€ config.py
â”‚       â””â”€â”€ config.yaml
â”‚
â”œâ”€â”€ .env                           # Environment variables
â”œâ”€â”€ Makefile                       # Automate pipelines (install, data, train, inference)
â””â”€â”€ README.md                      # Project documentation
```

---

## âš™ï¸ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/Telco-Customer-Churn-Prediction.git
   cd Telco-Customer-Churn-Prediction
   ```

2. Create and activate virtual environment:

   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Linux/Mac
   .venv\Scripts\activate      # Windows
   ```

Perfect ğŸ‘ thanks for clarifying! That means we should mention **two separate requirements files** in the README:

* One for running **Jupyter notebooks** in the `EDA/` folder.
* Another for running the **Python scripts & pipelines** in the `Python_Script/` folder.

Hereâ€™s the corrected part of the `README.md` (only the **Installation** section needs updating):

---

## âš™ï¸ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/Telco-Customer-Churn-Prediction.git
   cd Telco-Customer-Churn-Prediction
   ```

2. Create and activate a virtual environment:

   ```bash
   python -m venv .venv
   source .venv/bin/activate   # Linux/Mac
   .venv\Scripts\activate      # Windows
   ```

3. Install dependencies:

   * For **EDA notebooks**:

     ```bash
     pip install -r EDA/requirements.txt
     ```

   * For **Python scripts & pipelines**:

     ```bash
     pip install -r Python_Script/requirements.txt
     ```


---

## ğŸ› ï¸ Usage

### Run Pipelines with Makefile

```bash
make install             # Set up environment and dependencies
make data-pipeline       # Run data preprocessing
make train-pipeline      # Train models
make streaming-inference # Run inference on sample JSON
```

### Direct Execution

```bash
python Python_Script/pipelines/data_pipeline.py
python Python_Script/pipelines/training_pipeline.py
python Python_Script/pipelines/streaming_inference_pipeline.py
```

---

## ğŸ“ˆ Future Work

* Integration with **MLflow & ZenML** for experiment tracking
* Orchestration with **Airflow**
* Real-time data streaming with **Kafka**
* Deployment on **AWS with REST API**
* Monitoring for production-readiness

---

## ğŸ“œ License

This project is licensed under the MIT License.

---


